{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a41d8d2",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "- Gradient Boosting is an ensemble machine learning algorithm that builds a sequence of weak learners, typically decision trees, where each subsequent model tries to correct the errors of the previous models.\n",
    "\n",
    "- It optimizes a loss function by iteratively adding models that minimize the error, producing a strong predictive model.\n",
    "\n",
    "- Gradient Boosting is effective for both classification and regression problems and often yields high accuracy.\n",
    "\n",
    "- Unlike Random Forest which builds trees independently, Gradient Boosting builds trees sequentially, making it more prone to overfitting but also capable of capturing complex patterns.\n",
    "\n",
    "- Hyperparameters like learning rate, number of trees (n_estimators), and max depth are critical and require tuning.\n",
    "\n",
    "- Gradient Boosting can be slower to train but usually produces more accurate models for structured data.\n",
    "\n",
    "- It handles numerical and categorical data with appropriate preprocessing and supports custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/win10/Desktop/Project_Aug25/data/accidents_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361653d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "target = 'Severity'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeae25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64', 'bool']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric transformer pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # fill missing\n",
    "    ('scaler', StandardScaler())                     # scale numeric\n",
    "])\n",
    "\n",
    "# Categorical transformer pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for numeric and categorical\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8730b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with GradientBoostingClassifier instead of RandomForest\n",
    "clf_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42, n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Gradient Boosting model\n",
    "clf_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de131b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_gb = clf_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1af9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance extraction after preprocessing\n",
    "\n",
    "## Extract feature names after OneHotEncoding\n",
    "cat_features = clf.named_steps['preprocessor'].named_transformers_['cat'].\\\n",
    "  .named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "all_features = np.concatenate([numerical_cols, cat_features])\n",
    "\n",
    "importances = clf.named_steps['classifier'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Feature Importances from Random Forest Classifier\")\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), all_features[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ca90d",
   "metadata": {},
   "source": [
    "#### Task: Batch Training of Advanced Models\n",
    "- Explore and implement a method to train advanced machine learning models by dividing the preprocessed dataset into smaller batches.\n",
    "\n",
    "- Train the model incrementally on these batches rather than all data at once.\n",
    "\n",
    "- Combine or update the model progressively to obtain a final, fully trained model after processing all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24dab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
