{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a41d8d2",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "- Gradient Boosting is an ensemble machine learning algorithm that builds a sequence of weak learners, typically decision trees, where each subsequent model tries to correct the errors of the previous models.\n",
    "\n",
    "- It optimizes a loss function by iteratively adding models that minimize the error, producing a strong predictive model.\n",
    "\n",
    "- Gradient Boosting is effective for both classification and regression problems and often yields high accuracy.\n",
    "\n",
    "- Unlike Random Forest which builds trees independently, Gradient Boosting builds trees sequentially, making it more prone to overfitting but also capable of capturing complex patterns.\n",
    "\n",
    "- Hyperparameters like learning rate, number of trees (n_estimators), and max depth are critical and require tuning.\n",
    "\n",
    "- Gradient Boosting can be slower to train but usually produces more accurate models for structured data.\n",
    "\n",
    "- It handles numerical and categorical data with appropriate preprocessing and supports custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/win10/Desktop/Project_Aug25/data/accidents_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361653d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "target = 'Severity'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeae25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64', 'bool']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric transformer pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # fill missing\n",
    "    ('scaler', StandardScaler())                     # scale numeric\n",
    "])\n",
    "\n",
    "# Categorical transformer pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for numeric and categorical\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8730b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with GradientBoostingClassifier instead of RandomForest\n",
    "clf_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42, n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Gradient Boosting model\n",
    "clf_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de131b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_gb = clf_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1af9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance extraction after preprocessing\n",
    "\n",
    "## Extract feature names after OneHotEncoding\n",
    "cat_features = clf.named_steps['preprocessor'].named_transformers_['cat'].\\\n",
    "  .named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "all_features = np.concatenate([numerical_cols, cat_features])\n",
    "\n",
    "importances = clf.named_steps['classifier'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Feature Importances from Random Forest Classifier\")\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), all_features[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ca90d",
   "metadata": {},
   "source": [
    "#### Task: Batch Training of Advanced Models\n",
    "- Explore and implement a method to train advanced machine learning models by dividing the preprocessed dataset into smaller batches.\n",
    "\n",
    "- Train the model incrementally on these batches rather than all data at once.\n",
    "\n",
    "- Combine or update the model progressively to obtain a final, fully trained model after processing all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb24dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473994e",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c125f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/dinesh/Documents/Infosys Springboard/Dataset/gpt_CleanData/cleaned_us_accidents.csv\" \n",
    "target = \"Severity\" \n",
    "chunksize = 100_000     # smaller chunks to save RAM (was 200k)\n",
    "val_size = 20_000       # smaller validation sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eadbaf",
   "metadata": {},
   "source": [
    "Detect column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a9c3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 23 numerical and 10 categorical columns.\n"
     ]
    }
   ],
   "source": [
    "sample_df = pd.read_csv(data_path, nrows=5000)\n",
    "if target in sample_df.columns:\n",
    "    sample_df = sample_df.drop(columns=[target])\n",
    "\n",
    "categorical_cols = sample_df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = sample_df.select_dtypes(include=['int64', 'float64', 'bool']).columns.tolist()\n",
    "print(f\"Detected {len(numerical_cols)} numerical and {len(categorical_cols)} categorical columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9beaff6",
   "metadata": {},
   "source": [
    "Preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd1a0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=False))  # avoids dense array\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f53af8",
   "metadata": {},
   "source": [
    "Prepare validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "197948d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting preprocessor on validation sample...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv(data_path, nrows=val_size)\n",
    "X_val = val_df.drop(columns=[target])\n",
    "y_val = val_df[target]\n",
    "\n",
    "print(\"Fitting preprocessor on validation sample...\")\n",
    "X_val_trans = preprocessor.fit_transform(X_val)   # fit once\n",
    "\n",
    "# Map labels to 0..num_class-1\n",
    "unique_classes = sorted(y_val.unique())\n",
    "label_map = {label: i for i, label in enumerate(unique_classes)}\n",
    "y_val_mapped = y_val.map(label_map)\n",
    "\n",
    "# Convert to float32 and sparse DMatrix\n",
    "X_val_trans = X_val_trans.astype(np.float32)\n",
    "dval = xgb.DMatrix(X_val_trans, label=y_val_mapped)\n",
    "\n",
    "# Clean memory\n",
    "del X_val, val_df, X_val_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4929bc1",
   "metadata": {},
   "source": [
    "XGBoost parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "066ccc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(unique_classes),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',   # efficient on low RAM\n",
    "    'device': 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e31ce",
   "metadata": {},
   "source": [
    "Incremental training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45d7d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Batch 1...\n",
      "Batch 1 Validation Accuracy: 0.8809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.89      0.99      0.94     17432\n",
      "           2       0.68      0.18      0.28      1695\n",
      "           3       0.75      0.01      0.01       855\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.58      0.29      0.31     20000\n",
      "weighted avg       0.86      0.88      0.84     20000\n",
      "\n",
      "\n",
      "Processing Batch 2...\n",
      "Batch 2 Validation Accuracy: 0.8843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.89      0.99      0.94     17432\n",
      "           2       0.65      0.27      0.38      1695\n",
      "           3       0.72      0.02      0.03       855\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.57      0.32      0.34     20000\n",
      "weighted avg       0.86      0.88      0.85     20000\n",
      "\n",
      "\n",
      "Processing Batch 3...\n",
      "Batch 3 Validation Accuracy: 0.8855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.90      0.99      0.94     17432\n",
      "           2       0.64      0.31      0.41      1695\n",
      "           3       0.68      0.02      0.03       855\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.55      0.33      0.35     20000\n",
      "weighted avg       0.86      0.89      0.85     20000\n",
      "\n",
      "\n",
      "Processing Batch 4...\n",
      "Batch 4 Validation Accuracy: 0.8877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.90      0.99      0.94     17432\n",
      "           2       0.66      0.31      0.43      1695\n",
      "           3       0.63      0.03      0.06       855\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.55      0.33      0.36     20000\n",
      "weighted avg       0.87      0.89      0.86     20000\n",
      "\n",
      "\n",
      "Processing Batch 5...\n",
      "Batch 5 Validation Accuracy: 0.8878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.90      0.99      0.94     17432\n",
      "           2       0.66      0.32      0.43      1695\n",
      "           3       0.54      0.04      0.07       855\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.52      0.33      0.36     20000\n",
      "weighted avg       0.86      0.89      0.86     20000\n",
      "\n",
      "\n",
      "Processing Batch 6...\n",
      "Batch 6 Validation Accuracy: 0.8876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.90      0.99      0.94     17432\n",
      "           2       0.66      0.31      0.42      1695\n",
      "           3       0.56      0.04      0.08       855\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.53      0.33      0.36     20000\n",
      "weighted avg       0.86      0.89      0.86     20000\n",
      "\n",
      "\n",
      "Training complete — Incremental XGBoost finished successfully.\n"
     ]
    }
   ],
   "source": [
    "bst = None\n",
    "batch_i = 0\n",
    "\n",
    "for chunk in pd.read_csv(data_path, chunksize=chunksize, skiprows=range(1, val_size+1), header=0):\n",
    "    batch_i += 1\n",
    "    print(f\"\\nProcessing Batch {batch_i}...\")\n",
    "\n",
    "    # Features and labels\n",
    "    X_chunk = chunk.drop(columns=[target])\n",
    "    y_chunk = chunk[target].map(label_map)\n",
    "\n",
    "    # Drop invalid labels\n",
    "    valid_mask = y_chunk.notna()\n",
    "    X_chunk = X_chunk[valid_mask]\n",
    "    y_chunk = y_chunk[valid_mask]\n",
    "\n",
    "    # Transform features\n",
    "    X_chunk_trans = preprocessor.transform(X_chunk).astype(np.float32)\n",
    "    dtrain = xgb.DMatrix(X_chunk_trans, label=y_chunk)\n",
    "\n",
    "    del X_chunk, X_chunk_trans, chunk\n",
    "    gc.collect()\n",
    "\n",
    "    # Incremental training\n",
    "    if bst is None:\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=50)\n",
    "    else:\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=50, xgb_model=bst)\n",
    "\n",
    "    # Evaluate on validation\n",
    "    preds = bst.predict(dval)\n",
    "    acc = accuracy_score(y_val_mapped, preds)\n",
    "    print(f\"Batch {batch_i} Validation Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_val_mapped, preds, zero_division=0))\n",
    "\n",
    "print(\"\\nTraining complete — Incremental XGBoost finished successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
